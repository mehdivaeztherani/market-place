import instaloader
import time
import browser_cookie3
from instaloader.exceptions import ConnectionException, ProfileNotExistsException, QueryReturnedBadRequestException
import os
import shutil
from dotenv import load_dotenv
from io import BytesIO
import requests
from elevenlabs.client import ElevenLabs
import mysql.connector
from mysql.connector import Error
from datetime import datetime
import concurrent.futures
from threading import Lock
import glob
import random
from groq import Groq

load_dotenv()

# ElevenLabs API key
api_key = os.getenv("MY_API_KEY")

# Groq API key
GROQ_API_KEY = os.getenv("GROQ_API_KEY")

# Database configuration
DB_CONFIG = {
    'host': os.getenv('DB_HOST', 'turntable.proxy.rlwy.net'),
    'port': int(os.getenv('DB_PORT', '42664')),
    'user': os.getenv('DB_USER', 'root'),
    'password': os.getenv('DB_PASSWORD', 'OlWIFZHFiPpWIXCfaWdBLhILYxoqgecm'),
    'database': os.getenv('DB_NAME', 'railway'),
    'charset': 'utf8mb4',
    'collation': 'utf8mb4_unicode_ci',
    'ssl_disabled': False,
    'autocommit': True
}

# Global variables for thread safety
db_lock = Lock()
file_lock = Lock()

try:
    elevenlabs = ElevenLabs(api_key=api_key)
    print("âœ… ElevenLabs client initialized successfully")
except Exception as e:
    print(f"âŒ Error initializing ElevenLabs client: {str(e)}")
    elevenlabs = None

try:
    groq_client = Groq(api_key=GROQ_API_KEY)
    print("âœ… Groq AI client initialized successfully")
except Exception as e:
    print(f"âŒ Error initializing Groq client: {str(e)}")
    groq_client = None

def clear_instaloader_sessions():
    """Clear all Instaloader session files"""
    try:
        session_locations = [
            os.path.expanduser("~/.config/instaloader/"),
            os.path.expanduser("~/.instaloader/"),
            "./",
            os.getcwd()
        ]
        
        print("ğŸ§¹ Clearing old Instaloader sessions...")
        
        for location in session_locations:
            if os.path.exists(location):
                session_files = glob.glob(os.path.join(location, "session-*"))
                for session_file in session_files:
                    try:
                        os.remove(session_file)
                        print(f"ğŸ—‘ï¸  Removed: {session_file}")
                    except Exception as e:
                        print(f"âš ï¸  Could not remove {session_file}: {e}")
        
        print("âœ… Session cleanup completed")
        
    except Exception as e:
        print(f"âš ï¸  Error during session cleanup: {e}")

def generate_persian_title_with_ai(content, caption="", agent_name=""):
    """Generate Persian title using Groq AI"""
    if not groq_client or not content:
        print("âš ï¸ No Groq client or content available for title generation")
        return None
    
    try:
        print("ğŸ¤– Generating Persian title with AI...")
        print(f"ğŸ“ Content preview: {content[:100]}...")
        
        # Enhanced prompt for better title generation
        prompt = f"""ØªÙˆ ÛŒÚ© Ù…ØªØ®ØµØµ ØªÙˆÙ„ÛŒØ¯ Ø¹Ù†ÙˆØ§Ù† Ø¨Ø±Ø§ÛŒ Ø¢Ú¯Ù‡ÛŒâ€ŒÙ‡Ø§ÛŒ Ø§Ù…Ù„Ø§Ú© Ø¯Ø± Ø¯Ø¨ÛŒ Ù‡Ø³ØªÛŒ Ú©Ù‡ Ø¨Ø§ÛŒØ¯ Ø¹Ù†ÙˆØ§Ù†â€ŒÙ‡Ø§ÛŒ Ø¬Ø°Ø§Ø¨ Ùˆ Ù…Ù†Ø­ØµØ± Ø¨Ù‡ ÙØ±Ø¯ Ø¨Ø³Ø§Ø²ÛŒ.

ÙˆØ¸ÛŒÙÙ‡ ØªÙˆ:
1. Ø¨Ø± Ø§Ø³Ø§Ø³ Ù…Ø­ØªÙˆØ§ÛŒ Ø§Ø±Ø§Ø¦Ù‡ Ø´Ø¯Ù‡ØŒ ÛŒÚ© Ø¹Ù†ÙˆØ§Ù† Ø¬Ø°Ø§Ø¨ØŒ Ù…Ù†Ø­ØµØ± Ø¨Ù‡ ÙØ±Ø¯ Ùˆ Ø­Ø±ÙÙ‡â€ŒØ§ÛŒ Ø¨Ù‡ Ø²Ø¨Ø§Ù† ÙØ§Ø±Ø³ÛŒ Ø¨Ù†ÙˆÛŒØ³
2. Ø¹Ù†ÙˆØ§Ù† Ø¨Ø§ÛŒØ¯ Ø¨ÛŒÙ† 6 ØªØ§ 15 Ú©Ù„Ù…Ù‡ Ø¨Ø§Ø´Ø¯
3. Ø¹Ù†ÙˆØ§Ù† Ø¨Ø§ÛŒØ¯ Ø´Ø§Ù…Ù„ Ù†ÙˆØ¹ Ù…Ù„Ú© (Ø¢Ù¾Ø§Ø±ØªÙ…Ø§Ù†ØŒ ÙˆÛŒÙ„Ø§ØŒ Ø¯ÙØªØ±ØŒ Ù¾Ù†Øªâ€ŒÙ‡Ø§ÙˆØ³ Ùˆ...) Ùˆ Ù…Ù†Ø·Ù‚Ù‡ Ø¯Ø¨ÛŒ Ø¨Ø§Ø´Ø¯
4. Ø§Ø² Ú©Ù„Ù…Ø§Øª Ø¬Ø°Ø§Ø¨ Ù…Ø«Ù„ "Ù„ÙˆÚ©Ø³"ØŒ "Ù…Ù†Ø­ØµØ± Ø¨Ù‡ ÙØ±Ø¯"ØŒ "ÙˆÛŒÚ˜Ù‡"ØŒ "Ø§Ø³ØªØ«Ù†Ø§ÛŒÛŒ"ØŒ "Ø¨Ø±ØªØ±"ØŒ "ÙÙˆÙ‚â€ŒØ§Ù„Ø¹Ø§Ø¯Ù‡" Ø§Ø³ØªÙØ§Ø¯Ù‡ Ú©Ù†
5. Ø¹Ù†ÙˆØ§Ù† Ø¨Ø§ÛŒØ¯ Ø¨Ø±Ø§ÛŒ Ø¨Ø§Ø²Ø§Ø±ÛŒØ§Ø¨ÛŒ Ø§Ù…Ù„Ø§Ú© Ùˆ SEO Ù…Ù†Ø§Ø³Ø¨ Ø¨Ø§Ø´Ø¯
6. Ø¹Ù†ÙˆØ§Ù† Ø¨Ø§ÛŒØ¯ Ù…Ù†Ø¹Ú©Ø³â€ŒÚ©Ù†Ù†Ø¯Ù‡ Ù…Ø­ØªÙˆØ§ÛŒ ÙˆØ§Ù‚Ø¹ÛŒ Ù¾Ø³Øª Ø¨Ø§Ø´Ø¯
7. ÙÙ‚Ø· Ø¹Ù†ÙˆØ§Ù† Ø±Ø§ Ø¨Ù†ÙˆÛŒØ³ØŒ Ù‡ÛŒÚ† ØªÙˆØ¶ÛŒØ­ØŒ Ø¹Ù„Ø§Ù…Øª Ù†Ù‚Ù„ Ù‚ÙˆÙ„ ÛŒØ§ Ù…ØªÙ† Ø§Ø¶Ø§ÙÛŒ Ù†Ø¯Ù‡
8. Ø¹Ù†ÙˆØ§Ù† Ø¨Ø§ÛŒØ¯ Ú©Ø§Ù…Ù„Ø§Ù‹ Ù…Ù†Ø­ØµØ± Ø¨Ù‡ ÙØ±Ø¯ Ùˆ Ù…Ø±ØªØ¨Ø· Ø¨Ø§ Ù…Ø­ØªÙˆØ§ Ø¨Ø§Ø´Ø¯
{content[:500]}

Ú©Ù¾Ø´Ù† Ø§ØµÙ„ÛŒ:
{caption[:200] if caption else "Ø¨Ø¯ÙˆÙ† Ú©Ù¾Ø´Ù†"}

Ø¹Ù†ÙˆØ§Ù† ÙØ§Ø±Ø³ÛŒ:"""

        response = groq_client.chat.completions.create(
            messages=[
                {
                    "role": "user",
                    "content": prompt
                }
            ],
            model="gemma2-9b-it",  # Using a more powerful model
            temperature=0.7,
            max_tokens=150,
            top_p=0.9
        )
        
        title = response.choices[0].message.content.strip()
        
        # Clean up the title
        title = title.replace('"', '').replace("'", '').replace('Â«', '').replace('Â»', '').strip()
        
        # Remove any prefixes like "Ø¹Ù†ÙˆØ§Ù†:" or "Title:"
        if ':' in title:
            title = title.split(':', 1)[-1].strip()
        
        # Validate title length
        if len(title.split()) < 4 or len(title.split()) > 20:
            print("âš ï¸ AI title seems invalid, using fallback")
            return None
        
        # Check if title is meaningful (not just generic)
        generic_words = ['Ø§Ù…Ù„Ø§Ú©', 'ÙˆÛŒÚ˜Ù‡', 'Ø´Ù…Ø§Ø±Ù‡', 'Ù¾Ø³Øª', 'Ù…Ø­ØªÙˆØ§']
        if all(word in title for word in generic_words[:3]):
            print("âš ï¸ AI title seems too generic, regenerating...")
            return None
        
        print(f"âœ… Persian title generated: {title}")
        return title
        
    except Exception as e:
        print(f"âš ï¸ AI title generation failed: {e}")
        print(f"ğŸ” Error details: {str(e)}")
        return None

def clean_transcription_with_ai(original_transcription):
    """Clean transcription using Groq AI"""
    if not groq_client or not original_transcription:
        return original_transcription
    
    try:
        print("ğŸ¤– Cleaning transcription with AI...")
        
        # Enhanced prompt for better transcription cleaning
        prompt = f"""ØªÙˆ ÛŒÚ© ÙˆÛŒØ±Ø§Ø³ØªØ§Ø± Ø­Ø±ÙÙ‡â€ŒØ§ÛŒ Ùˆ Ø¨Ø§ØªØ¬Ø±Ø¨Ù‡ Ù…Ø­ØªÙˆØ§ Ø¨Ø±Ø§ÛŒ ÙˆØ¨Ø³Ø§ÛŒØª Ùˆ ÙˆØ¨Ù„Ø§Ú¯ Ù‡Ø³ØªÛŒ. Ù…ØªÙ† Ø²ÛŒØ± ØªØ±Ù†Ø³Ú©Ø±ÛŒÙ¾Ø´Ù† ÛŒÚ© ÙˆÛŒØ¯ÛŒÙˆ Ø§Ø² Ø§ÛŒÙ†Ø³ØªØ§Ú¯Ø±Ø§Ù… Ø§Ø³Øª.
ÙˆØ¸ÛŒÙÙ‡ ØªÙˆ:
1. Ù…ØªÙ† Ø±Ø§ Ø¨Ø±Ø§ÛŒ Ø§Ù†ØªØ´Ø§Ø± Ø¯Ø± ÙˆØ¨Ø³Ø§ÛŒØª Ùˆ ÙˆØ¨Ù„Ø§Ú¯ Ø¨Ù‡â€ŒØµÙˆØ±Øª Ø­Ø±ÙÙ‡â€ŒØ§ÛŒØŒ Ø±ÙˆØ§Ù† Ùˆ Ø®ÙˆØ§Ù†Ø§ ÙˆÛŒØ±Ø§ÛŒØ´ Ú©Ù†.
2. Ù…Ø­ØªÙˆØ§ÛŒ Ø§ØµÙ„ÛŒ Ùˆ Ù¾ÛŒØ§Ù… Ú©Ù„ÛŒØ¯ÛŒ Ù…ØªÙ† Ø±Ø§ Ú©Ø§Ù…Ù„Ø§Ù‹ Ø­ÙØ¸ Ú©Ù†ØŒ Ú†Ù‡ Ù…Ø±Ø¨ÙˆØ· Ø¨Ù‡ Ø§Ù…Ù„Ø§Ú© Ø¨Ø§Ø´Ø¯ Ùˆ Ú†Ù‡ Ù…ÙˆØ¶ÙˆØ¹ Ø¯ÛŒÚ¯Ø±ÛŒ.
3. Ú©Ù„Ù…Ø§Øª Ù†Ø§Ù…Ù†Ø§Ø³Ø¨ØŒ ØªÚ©Ø±Ø§Ø±Ù‡Ø§ÛŒ ØºÛŒØ±Ø¶Ø±ÙˆØ±ÛŒØŒ Ø¹Ø¨Ø§Ø±Ø§Øª ØºÛŒØ±Ø­Ø±ÙÙ‡â€ŒØ§ÛŒ ÛŒØ§ Ù…Ø­Ø§ÙˆØ±Ù‡â€ŒØ§ÛŒ Ø±Ø§ Ø­Ø°Ù ÛŒØ§ Ø§ØµÙ„Ø§Ø­ Ú©Ù†.
4. Ø¬Ù…Ù„Ø§Øª Ù†Ø§ØªÙ…Ø§Ù… ÛŒØ§ Ù…Ø¨Ù‡Ù… Ø±Ø§ ÙˆØ§Ø¶Ø­ Ùˆ Ú©Ø§Ù…Ù„ Ú©Ù†ØŒ Ø¨Ø¯ÙˆÙ† ØªØºÛŒÛŒØ± Ø¯Ø± Ù…Ø¹Ù†Ø§ÛŒ Ø§ØµÙ„ÛŒ.
5. ØªÙ…Ø§Ù… Ø§Ø·Ù„Ø§Ø¹Ø§Øª Ú©Ù„ÛŒØ¯ÛŒ Ù…Ø§Ù†Ù†Ø¯ Ø¬Ø²Ø¦ÛŒØ§Øª ØªÙ…Ø§Ø³ØŒ Ù‚ÛŒÙ…Øªâ€ŒÙ‡Ø§ (Ø¯Ø± ØµÙˆØ±Øª ÙˆØ¬ÙˆØ¯) Ùˆ Ø³Ø§ÛŒØ± Ø§Ø·Ù„Ø§Ø¹Ø§Øª Ù…Ù‡Ù… Ø±Ø§ Ø¯Ù‚ÛŒÙ‚Ø§Ù‹ Ø­ÙØ¸ Ú©Ù†.
6. Ø³Ø¨Ú© Ù†ÙˆØ´ØªØ§Ø± Ø±Ø§ Ø¨Ù‡â€ŒÚ¯ÙˆÙ†Ù‡â€ŒØ§ÛŒ ØªÙ†Ø¸ÛŒÙ… Ú©Ù† Ú©Ù‡ Ø¨Ø±Ø§ÛŒ Ù…Ø®Ø§Ø·Ø¨Ø§Ù† ÙˆØ¨Ø³Ø§ÛŒØª Ø­Ø±ÙÙ‡â€ŒØ§ÛŒ Ùˆ Ø¬Ø°Ø§Ø¨ Ø¨Ø§Ø´Ø¯.
7. ÙÙ‚Ø· Ù…ØªÙ† ÙˆÛŒØ±Ø§ÛŒØ´â€ŒØ´Ø¯Ù‡ Ø±Ø§ Ø§Ø±Ø§Ø¦Ù‡ Ú©Ù†ØŒ Ø¨Ø¯ÙˆÙ† Ù‡ÛŒÚ† ØªÙˆØ¶ÛŒØ­ Ø§Ø¶Ø§ÙÛŒ.

Ù…ØªÙ† Ø§ØµÙ„ÛŒ:
{original_transcription}

Ù…ØªÙ† ØªÙ…ÛŒØ² Ø´Ø¯Ù‡:"""

        response = groq_client.chat.completions.create(
            messages=[
                {
                    "role": "user",
                    "content": prompt
                }
            ],
            model="gemma2-9b-it",  # Using a good model for Persian text
            temperature=0.3,  # Lower temperature for more consistent cleaning
            max_tokens=2000,
            top_p=0.9
        )
        
        cleaned_text = response.choices[0].message.content.strip()
        
        # Basic validation - ensure we got meaningful content back
        if len(cleaned_text) < 20 or len(cleaned_text) > len(original_transcription) * 2:
            print("âš ï¸ AI cleaning result seems invalid, using original")
            return original_transcription
        
        print("âœ… Transcription cleaned successfully with AI")
        return cleaned_text
        
    except Exception as e:
        print(f"âš ï¸ AI cleaning failed: {e}")
        print("ğŸ“ Using original transcription")
        return original_transcription

def create_database_connection():
    """Create and return a database connection"""
    try:
        connection = mysql.connector.connect(**DB_CONFIG)
        if connection.is_connected():
            return connection
    except Error as e:
        print(f"âŒ Error connecting to MySQL database: {e}")
        return None

def ensure_database_structure(connection):
    """Ensure the database has the correct structure - OPTIMIZED"""
    try:
        cursor = connection.cursor()

        cursor.execute("""
            SELECT COLUMN_NAME
            FROM INFORMATION_SCHEMA.COLUMNS
            WHERE TABLE_SCHEMA = %s AND TABLE_NAME = 'posts'
            AND COLUMN_NAME IN ('thumbnail', 'instagram_shortcode')
        """, (DB_CONFIG['database'],))

        existing_columns = {row[0] for row in cursor.fetchall()}

        if 'thumbnail' not in existing_columns:
            cursor.execute("ALTER TABLE posts ADD COLUMN thumbnail VARCHAR(255) AFTER caption")

        if 'instagram_shortcode' not in existing_columns:
            cursor.execute("ALTER TABLE posts ADD COLUMN instagram_shortcode VARCHAR(50) AFTER original_url")
            cursor.execute("CREATE UNIQUE INDEX idx_agent_shortcode ON posts (agent_id, instagram_shortcode)")

        cursor.execute("""
            SELECT TABLE_NAME FROM INFORMATION_SCHEMA.TABLES
            WHERE TABLE_SCHEMA = %s AND TABLE_NAME = 'filtered_posts'
        """, (DB_CONFIG['database'],))

        if not cursor.fetchone():
            cursor.execute("""
                CREATE TABLE filtered_posts (
                    id INT AUTO_INCREMENT PRIMARY KEY,
                    agent_id VARCHAR(50),
                    instagram_shortcode VARCHAR(50),
                    filter_reason VARCHAR(100),
                    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                    UNIQUE KEY unique_agent_shortcode (agent_id, instagram_shortcode)
                )
            """)

        connection.commit()
        return True
    except Error as e:
        print(f"âŒ Error checking database structure: {e}")
        return False
    finally:
        if cursor:
            cursor.close()

def count_persian_characters(text):
    """Count Persian/Farsi characters in text - OPTIMIZED"""
    if not text:
        return 0

    persian_chars = set('Ø¢Ø§Ø¨Ù¾ØªØ«Ø¬Ú†Ø­Ø®Ø¯Ø°Ø±Ø²Ú˜Ø³Ø´ØµØ¶Ø·Ø¸Ø¹ØºÙÙ‚Ú©Ú¯Ù„Ù…Ù†ÙˆÙ‡ÛŒ')
    return sum(1 for char in text if char in persian_chars)

def get_or_create_agent(connection, username, profile_data):
    """Get existing agent or create new one - OPTIMIZED"""
    try:
        cursor = connection.cursor()

        cursor.execute("SELECT id FROM agents WHERE instagram = %s", (username,))
        existing_agent = cursor.fetchone()

        if existing_agent:
            return existing_agent[0]

        timestamp = int(time.time())
        agent_id = f"{username.replace('.', '-').replace('_', '-')}-{timestamp}"

        full_name = profile_data.get('full_name', '').strip() or username.replace('.', ' ').replace('_', ' ').title()
        biography = profile_data.get('biography', '').strip() or f'Ù…Ø´Ø§ÙˆØ± Ø§Ù…Ù„Ø§Ú© Ø­Ø±ÙÙ‡â€ŒØ§ÛŒ Ø¯Ø± Ø¯Ø¨ÛŒ. Ø¨Ø±Ø§ÛŒ Ø¢Ø®Ø±ÛŒÙ† Ø¨Ù‡â€ŒØ±ÙˆØ²Ø±Ø³Ø§Ù†ÛŒâ€ŒÙ‡Ø§ÛŒ Ø§Ù…Ù„Ø§Ú© @{username} Ø±Ø§ Ø¯Ù†Ø¨Ø§Ù„ Ú©Ù†ÛŒØ¯.'

        cursor.execute("""
            INSERT INTO agents (id, name, profile_image, address, bio, phone, email, instagram, created_at)
            VALUES (%s, %s, %s, %s, %s, %s, %s, %s, NOW())
        """, (
            agent_id, full_name, f"/agents/{agent_id}/profile/profile_picture.jpg",
            "Ø¯Ø¨ÛŒØŒ Ø§Ù…Ø§Ø±Ø§Øª Ù…ØªØ­Ø¯Ù‡ Ø¹Ø±Ø¨ÛŒ", biography, None, None, username
        ))

        connection.commit()
        print(f"âœ… New agent created: {agent_id}")
        return agent_id

    except Error as e:
        print(f"âŒ Error getting/creating agent: {e}")
        connection.rollback()
        return None
    finally:
        if cursor:
            cursor.close()

def get_existing_and_filtered_shortcodes(connection, agent_id):
    """Get both existing and filtered shortcodes in one query - OPTIMIZED"""
    try:
        cursor = connection.cursor()

        cursor.execute("SELECT instagram_shortcode FROM posts WHERE agent_id = %s AND instagram_shortcode IS NOT NULL", (agent_id,))
        existing = {row[0] for row in cursor.fetchall()}

        cursor.execute("SELECT instagram_shortcode FROM filtered_posts WHERE agent_id = %s", (agent_id,))
        filtered = {row[0] for row in cursor.fetchall()}

        return existing, filtered

    except Error as e:
        print(f"âŒ Error getting shortcodes: {e}")
        return set(), set()
    finally:
        if cursor:
            cursor.close()

def add_to_filtered_posts(connection, agent_id, shortcode, reason):
    """Add a post to the filtered posts table - OPTIMIZED"""
    try:
        cursor = connection.cursor()
        cursor.execute("""
            INSERT IGNORE INTO filtered_posts (agent_id, instagram_shortcode, filter_reason)
            VALUES (%s, %s, %s)
        """, (agent_id, shortcode, reason))
        connection.commit()
    except Error as e:
        print(f"âŒ Error adding to filtered posts: {e}")
    finally:
        if cursor:
            cursor.close()

def get_next_post_number(connection, agent_id):
    """Get the next post number - OPTIMIZED"""
    try:
        cursor = connection.cursor()
        cursor.execute("SELECT COUNT(*) FROM posts WHERE agent_id = %s", (agent_id,))
        return cursor.fetchone()[0] + 1
    except Error as e:
        return 1
    finally:
        if cursor:
            cursor.close()

def save_post_to_database(connection, agent_id, post_data, post_number):
    """Save a post to the database with AI-generated title - OPTIMIZED"""
    try:
        with db_lock:
            cursor = connection.cursor()

            timestamp = int(time.time())
            post_id = f"{agent_id}-post-{post_number:03d}-{timestamp}"

            cursor.execute("SELECT id FROM posts WHERE agent_id = %s AND instagram_shortcode = %s",
                          (agent_id, post_data.get('instagram_shortcode', '')))
            if cursor.fetchone():
                return "duplicate"

            # Always generate AI title based on content
            title = post_data.get('title', '')
            if not title:  # Only generate if no title provided
                print(f"ğŸ¤– Generating AI title for post {post_number}...")
                ai_title = generate_persian_title_with_ai(
                    post_data.get('content', ''),
                    post_data.get('caption', ''),
                    post_data.get('agent_name', '')
                )
                
                if ai_title:
                    title = ai_title
                    print(f"âœ… AI title generated: {title}")
                else:
                    # More descriptive fallback based on content
                    content_preview = post_data.get('content', '')[:100]
                    if 'Ø¢Ù¾Ø§Ø±ØªÙ…Ø§Ù†' in content_preview:
                        title = f"Ø¢Ù¾Ø§Ø±ØªÙ…Ø§Ù† Ù…Ù†Ø­ØµØ± Ø¨Ù‡ ÙØ±Ø¯ Ø¯Ø± Ø¯Ø¨ÛŒ - Ù¾Ø³Øª {post_number}"
                    elif 'ÙˆÛŒÙ„Ø§' in content_preview:
                        title = f"ÙˆÛŒÙ„Ø§ÛŒ Ù„ÙˆÚ©Ø³ Ø¯Ø± Ø¯Ø¨ÛŒ - Ù¾Ø³Øª {post_number}"
                    elif 'Ø¯ÙØªØ±' in content_preview:
                        title = f"Ø¯ÙØªØ± ØªØ¬Ø§Ø±ÛŒ Ù…Ø¯Ø±Ù† Ø¯Ø± Ø¯Ø¨ÛŒ - Ù¾Ø³Øª {post_number}"
                    else:
                        title = f"Ø§Ù…Ù„Ø§Ú© Ø§Ø³ØªØ«Ù†Ø§ÛŒÛŒ Ø¯Ø± Ø¯Ø¨ÛŒ - Ù¾Ø³Øª {post_number}"
                    print(f"âš ï¸ Using enhanced fallback title: {title}")

            cursor.execute("""
                INSERT INTO posts (id, agent_id, title, content, caption, thumbnail, transcription, date, original_url, instagram_shortcode, created_at)
                VALUES (%s, %s, %s, %s, %s, %s, %s, %s, %s, %s, NOW())
            """, (
                post_id, agent_id, title,
                post_data.get('content', ''), post_data.get('caption', ''),
                f"/agents/{agent_id}/posts/post_{post_number}_thumbnail.jpg",
                post_data.get('transcription', None), post_data.get('date', datetime.now()),
                post_data.get('original_url', ''), post_data.get('instagram_shortcode', '')
            ))

            connection.commit()
            print(f"âœ… Post saved with title: {title}")
            return post_id

    except Error as e:
        if "Duplicate entry" in str(e):
            return "duplicate"
        else:
            print(f"âŒ Error saving post: {e}")
            connection.rollback()
            return None
    finally:
        if cursor:
            cursor.close()

def download_and_process_media(post, post_folder, post_number, download_folder):
    """Download and process media files - OPTIMIZED"""
    try:
        thumbnail_response = requests.get(post.url, timeout=15)
        if thumbnail_response.status_code == 200:
            thumbnail_path = os.path.join(post_folder, f"post_{post_number}_thumbnail.jpg")
            with open(thumbnail_path, 'wb') as f:
                f.write(thumbnail_response.content)

        video_processed = False
        for file in os.listdir(download_folder):
            if file.endswith('.mp4') and 'UTC' in file:
                old_video_path = os.path.join(download_folder, file)
                new_video_path = os.path.join(post_folder, f"post_{post_number}_video.mp4")
                shutil.move(old_video_path, new_video_path)
                video_processed = True
                break

        return video_processed, new_video_path if video_processed else None

    except Exception as e:
        print(f"âš ï¸ Error downloading media: {e}")
        return False, None

def transcribe_video_optimized(video_path):
    """Transcribe video with optimized settings"""
    if not elevenlabs:
        return None

    try:
        with open(video_path, 'rb') as f:
            audio_data = BytesIO(f.read())

        transcription = elevenlabs.speech_to_text.convert(
            file=audio_data,
            model_id="scribe_v1",
            language_code="fas"
        )

        return transcription.text.strip()

    except Exception as e:
        print(f"âš ï¸ Transcription failed: {e}")
        return None

def process_single_post(post, agent_id, connection, download_folder, current_post_number, existing_shortcodes, filtered_shortcodes, profile_data, username):
    """Process a single post - WITH AI TRANSCRIPTION CLEANING AND TITLE GENERATION"""
    post_shortcode = post.shortcode

    if post_shortcode in existing_shortcodes or post_shortcode in filtered_shortcodes:
        return None, "skipped"

    try:
        post_folder_name = f"post_{current_post_number}_{post.date_utc.strftime('%Y%m%d_%H%M%S')}_{post_shortcode}"
        post_folder = os.path.join(download_folder, post_folder_name)

        with file_lock:
            os.makedirs(post_folder, exist_ok=True)

        # EXACT WORKING METHOD FROM YOUR ORIGINAL CODE
        ig = instaloader.Instaloader(
            download_videos=True,
            download_video_thumbnails=False,
            download_geotags=False,
            download_comments=False,
            save_metadata=False,
            compress_json=False,
            post_metadata_txt_pattern=""
        )

        cookies = browser_cookie3.chrome(domain_name="instagram.com")
        ig.context._session.cookies.update(cookies)

        ig.download_post(post, target=download_folder)

        video_processed, video_path = download_and_process_media(post, post_folder, current_post_number, download_folder)

        if not video_processed:
            add_to_filtered_posts(connection, agent_id, post_shortcode, "image_only_no_video")
            return None, "filtered"

        # Step 1: Get original transcription from ElevenLabs
        original_transcription = transcribe_video_optimized(video_path)

        if not original_transcription:
            add_to_filtered_posts(connection, agent_id, post_shortcode, "transcription_failed")
            return None, "filtered"

        persian_char_count = count_persian_characters(original_transcription)

        if persian_char_count < 50:
            add_to_filtered_posts(connection, agent_id, post_shortcode, f"insufficient_persian_chars_{persian_char_count}")
            return None, "filtered"

        # Step 2: Clean transcription with AI
        cleaned_transcription = clean_transcription_with_ai(original_transcription)

        # Save both original and cleaned transcriptions
        with open(os.path.join(post_folder, "transcription_original.txt"), 'w', encoding='utf-8') as f:
            f.write(original_transcription)

        with open(os.path.join(post_folder, "transcription_cleaned.txt"), 'w', encoding='utf-8') as f:
            f.write(cleaned_transcription)

        # For backward compatibility, also save as transcription.txt (cleaned version)
        with open(os.path.join(post_folder, "transcription.txt"), 'w', encoding='utf-8') as f:
            f.write(cleaned_transcription)

        caption = post.caption if post.caption else "Ø¨Ø¯ÙˆÙ† Ú©Ù¾Ø´Ù†"
        if post.caption_mentions:
            caption += f"\n\nğŸ‘¥ Ù…Ù†Ø´Ù†â€ŒÙ‡Ø§: {', '.join([f'@{mention}' for mention in post.caption_mentions])}"
        if post.caption_hashtags:
            caption += f"\n\nğŸ·ï¸ Ù‡Ø´ØªÚ¯â€ŒÙ‡Ø§: {', '.join([f'#{hashtag}' for hashtag in post.caption_hashtags])}"

        with open(os.path.join(post_folder, "caption.txt"), 'w', encoding='utf-8') as f:
            f.write(caption)

        # Step 3: Generate AI title based on cleaned content
        print(f"ğŸ¤– Generating unique AI title for post {current_post_number}...")
        ai_title = generate_persian_title_with_ai(
            cleaned_transcription,
            caption,
            profile_data['full_name'] or username
        )
        
        if ai_title:
            print(f"âœ… Generated unique title: {ai_title}")
        else:
            print(f"âš ï¸ AI title generation failed, will use enhanced fallback")
        # Use cleaned transcription for database content
        post_data = {
            'title': ai_title,  # Pass the AI-generated title
            'content': cleaned_transcription,  # Using cleaned version
            'caption': caption,
            'transcription': cleaned_transcription,  # Using cleaned version
            'date': post.date_utc,
            'original_url': f"https://instagram.com/p/{post_shortcode}",
            'instagram_shortcode': post_shortcode,
            'agent_name': profile_data['full_name'] or username
        }

        post_id = save_post_to_database(connection, agent_id, post_data, current_post_number)

        if post_id and post_id != "duplicate":
            return {
                'post_number': current_post_number,
                'folder_name': post_folder_name,
                'post_id': post_id,
                'shortcode': post_shortcode
            }, "success"

        return None, "failed"

    except Exception as e:
        print(f"âŒ Error processing post {post_shortcode}: {e}")
        return None, "error"

def organize_files_optimized(username, agent_id, downloaded_folder, saved_posts_info):
    """Organize files with optimized operations"""
    try:
        agent_public_dir = f"public/agents/{agent_id}"
        profile_dir = f"{agent_public_dir}/profile"
        posts_dir = f"{agent_public_dir}/posts"

        os.makedirs(profile_dir, exist_ok=True)
        os.makedirs(posts_dir, exist_ok=True)

        profile_pic_source = os.path.join(downloaded_folder, f"{username}_profile.jpg")
        if os.path.exists(profile_pic_source):
            shutil.copy2(profile_pic_source, os.path.join(profile_dir, "profile_picture.jpg"))

        def copy_post_files(post_info):
            post_number = post_info['post_number']
            post_folder_name = post_info['folder_name']
            post_folder_path = os.path.join(downloaded_folder, post_folder_name)

            if not os.path.exists(post_folder_path):
                return

            thumbnail_files = [f for f in os.listdir(post_folder_path) if f.endswith('_thumbnail.jpg')]
            if thumbnail_files:
                shutil.copy2(
                    os.path.join(post_folder_path, thumbnail_files[0]),
                    os.path.join(posts_dir, f"post_{post_number}_thumbnail.jpg")
                )

            video_files = [f for f in os.listdir(post_folder_path) if f.endswith('_video.mp4')]
            if video_files:
                shutil.copy2(
                    os.path.join(post_folder_path, video_files[0]),
                    os.path.join(posts_dir, f"post_{post_number}_video.mp4")
                )

            for file in os.listdir(post_folder_path):
                if file.endswith('.txt'):
                    shutil.copy2(
                        os.path.join(post_folder_path, file),
                        os.path.join(posts_dir, f"post_{post_number}_{file}")
                    )

        with concurrent.futures.ThreadPoolExecutor(max_workers=4) as executor:
            executor.map(copy_post_files, saved_posts_info)

        return True

    except Exception as e:
        print(f"âŒ Error organizing files: {e}")
        return False

def download_instagram_profile(username, browser="chrome", max_posts=5):
    """WORKING Instagram profile downloader WITH AI TRANSCRIPTION CLEANING AND TITLE GENERATION"""
    print(f"ğŸš€ Instagram scraper with AI cleaning and title generation for @{username}")
    print("âš¡ USING EXACT WORKING METHOD + AI TRANSCRIPTION CLEANING + PERSIAN TITLE GENERATION")
    print("ğŸ¤– AI will clean transcriptions and generate Persian titles for website/blog use")
    print("ğŸ“… POSTS ORDERED: Newest to Oldest")
    print("=" * 60)

    # Only add session cleanup - everything else stays the same
    clear_instaloader_sessions()

    connection = create_database_connection()
    if not connection:
        return

    if not ensure_database_structure(connection):
        return

    try:
        print(f"ğŸª Loading cookies from {browser}...")
        if browser.lower() == "chrome":
            cookies = browser_cookie3.chrome(domain_name="instagram.com")
        elif browser.lower() == "safari":
            cookies = browser_cookie3.safari(domain_name="instagram.com")
        else:
            cookies = browser_cookie3.chromium(domain_name="instagram.com")

        # EXACT WORKING METHOD FROM YOUR ORIGINAL CODE
        ig = instaloader.Instaloader(
            download_videos=True,
            download_video_thumbnails=False,
            download_geotags=False,
            download_comments=False,
            save_metadata=False,
            compress_json=False,
            post_metadata_txt_pattern=""
        )

        ig.context._session.cookies.update(cookies)

        download_folder = f"{username}_posts_{int(time.time())}"
        os.makedirs(download_folder, exist_ok=True)

        print(f"ğŸ” Fetching profile information...")
        profile = instaloader.Profile.from_username(ig.context, username)

        profile_data = {
            'full_name': profile.full_name or '',
            'biography': profile.biography or '',
            'followers': profile.followers,
            'mediacount': profile.mediacount
        }

        print(f"ğŸ‘¤ {profile_data['full_name']} - {profile_data['followers']:,} followers")

        agent_id = get_or_create_agent(connection, username, profile_data)
        if not agent_id:
            return

        existing_shortcodes, filtered_shortcodes = get_existing_and_filtered_shortcodes(connection, agent_id)

        print(f"ğŸ“Š Already have {len(existing_shortcodes)} posts in database")
        print(f"ğŸ“Š Already filtered {len(filtered_shortcodes)} posts")

        profile_pic_response = requests.get(profile.get_profile_pic_url(), timeout=15)
        if profile_pic_response.status_code == 200:
            with open(os.path.join(download_folder, f"{username}_profile.jpg"), 'wb') as f:
                f.write(profile_pic_response.content)

        print(f"ğŸ“¥ Getting posts from Instagram (this may take a moment)...")

        saved_posts_info = []
        successful_posts = 0
        skipped_posts = 0
        filtered_posts = 0
        total_checked = 0
        current_post_number = get_next_post_number(connection, agent_id)

        print(f"ğŸ¯ Looking for {max_posts} NEW posts...")
        print(f"ğŸ“‹ Starting from post number {current_post_number}")

        for post in profile.get_posts():
            if successful_posts >= max_posts:
                print(f"âœ… SUCCESS: Got {max_posts} new posts, stopping!")
                break

            total_checked += 1
            post_shortcode = post.shortcode
            post_date = post.date_utc.strftime('%Y-%m-%d %H:%M')

            if post_shortcode in existing_shortcodes:
                skipped_posts += 1
                print(f"â­ï¸  Skip #{total_checked} (exists): {post_date}")
                continue

            if post_shortcode in filtered_shortcodes:
                filtered_posts += 1
                print(f"ğŸš« Skip #{total_checked} (filtered): {post_date}")
                continue

            print(f"ğŸ†• NEW POST #{total_checked} ({successful_posts + 1}/{max_posts}): {post_date}")

            result, status = process_single_post(
                post, agent_id, connection, download_folder, current_post_number,
                existing_shortcodes, filtered_shortcodes, profile_data, username
            )

            if status == "success" and result:
                saved_posts_info.append(result)
                existing_shortcodes.add(result['shortcode'])
                successful_posts += 1
                current_post_number += 1
                print(f"âœ… SAVED: Post {successful_posts}/{max_posts} (AI cleaned + Persian title)")
            elif status == "filtered":
                filtered_shortcodes.add(post_shortcode)
                filtered_posts += 1
                print(f"ğŸš« FILTERED: Not suitable")
            else:
                print(f"âŒ FAILED: Could not process")

            # Add small random delay to avoid rate limiting
            time.sleep(random.uniform(0.5, 1.5))

        if successful_posts > 0:
            print(f"ğŸ“ Organizing {successful_posts} files...")
            organize_files_optimized(username, agent_id, download_folder, saved_posts_info)

        print(f"\nğŸ‰ FINAL SUMMARY")
        print("=" * 50)
        print(f"ğŸ‘¤ Agent: {profile_data['full_name']} (@{username})")
        print(f"ğŸ” Total posts checked: {total_checked}")
        print(f"â­ï¸  Skipped (already exist): {skipped_posts}")
        print(f"ğŸš« Skipped (filtered): {filtered_posts}")
        print(f"âœ… NEW posts saved: {successful_posts}")
        print(f"ğŸ¤– AI cleaned transcriptions: {successful_posts}")
        print(f"ğŸ·ï¸ AI generated Persian titles: {successful_posts}")
        print(f"ğŸ“Š Database now has: {len(existing_shortcodes)} total posts")
        print(f"ğŸ”„ CONTINUATION: âœ… WORKING")
        print(f"ğŸ“… Order: Newest to Oldest âœ…")

        if successful_posts < max_posts:
            print(f"âš ï¸  Note: Only found {successful_posts} new posts (requested {max_posts})")
            print("   This means you've reached older posts or end of profile")

        try:
            shutil.rmtree(download_folder)
        except:
            pass

    except QueryReturnedBadRequestException as e:
        print(f"âš ï¸ Rate limited or unauthorized: {e}")
        print("ğŸ’¡ Solutions:")
        print("   1. Wait 10-15 minutes before trying again")
        print("   2. Make sure you're logged into Instagram in your browser")
        print("   3. Try using a different browser (chrome/firefox/safari)")
        print("   4. Clear browser cache and cookies, then log in again")
    except Exception as e:
        print(f"âŒ Unexpected error: {e}")
    finally:
        if connection and connection.is_connected():
            connection.close()

def test_database_and_show_agents():
    """Test database connection - OPTIMIZED"""
    connection = create_database_connection()
    if connection:
        try:
            cursor = connection.cursor()
            cursor.execute("""
                SELECT
                    (SELECT COUNT(*) FROM agents) as agents,
                    (SELECT COUNT(*) FROM posts) as posts,
                    (SELECT COUNT(*) FROM filtered_posts) as filtered
            """)
            counts = cursor.fetchone()
            print(f"ğŸ“Š Database: {counts[0]} agents, {counts[1]} posts, {counts[2]} filtered")
        except Error as e:
            print(f"âŒ Database error: {e}")
        finally:
            cursor.close()
            connection.close()

if __name__ == "__main__":
    print("ğŸš€ INSTAGRAM SCRAPER WITH AI TRANSCRIPTION CLEANING AND PERSIAN TITLE GENERATION")
    print("=" * 60)
    print("âœ… USING EXACT WORKING METHOD FROM YOUR ORIGINAL CODE")
    print("ğŸ¤– NEW FEATURES:")
    print("â€¢ AI TRANSCRIPTION CLEANING")
    print("â€¢ AI PERSIAN TITLE GENERATION (UNIQUE & CONTENT-BASED)")
    print("ğŸ”„ GUARANTEED CONTINUATION:")
    print("â€¢ First run: Gets posts 1-5")
    print("â€¢ Second run: Gets posts 6-10")
    print("â€¢ Third run: Gets posts 11-15")
    print("â€¢ And so on...")
    print("ğŸ§¹ FEATURES:")
    print("â€¢ Session cleanup (clears old cookies)")
    print("â€¢ Random delays (avoids rate limits)")
    print("â€¢ AI transcription cleaning for website/blog")
    print("â€¢ AI Persian title generation (unique, content-based)")
    print("â€¢ Saves both original and cleaned transcriptions")
    print("ğŸ“… POST ORDERING: Newest to Oldest")
    print("ğŸ” FILTER: 50+ Persian characters only")
    print("ğŸ·ï¸ TITLE GENERATION: AI creates unique titles based on actual content")
    print("=" * 60)

    test_database_and_show_agents()

    print("\nğŸ’¡ BEFORE STARTING:")
    print("1. Make sure you're logged into Instagram in your browser")
    print("2. Close any Instagram tabs and reopen them")
    print("3. Make sure your Instagram account is not restricted")
    print("4. AI will clean transcriptions and generate UNIQUE Persian titles based on content")
    print("5. Make sure GROQ_API_KEY is set in your .env file for title generation")
    print("=" * 60)

    username = input("Enter Instagram username (default: mojtaba.dubai.amlak): ").strip() or "mojtaba.dubai.amlak"
    browser = input("Enter browser (chrome/firefox, default: chrome): ").strip() or "chrome"
    try:
        max_posts = int(input("Enter max NEW posts to get (default: 5): ").strip() or "5")
    except ValueError:
        max_posts = 5

    print(f"\nğŸ¯ Starting AI-enhanced scraper:")
    print(f"ğŸ“¸ Username: @{username}")
    print(f"ğŸŒ Browser: {browser}")
    print(f"ğŸ“Š Max NEW Posts: {max_posts}")
    print(f"ğŸ“… Order: Newest â†’ Oldest")
    print(f"ğŸ”„ Continuation: GUARANTEED")
    print(f"âœ… Method: EXACT WORKING ORIGINAL")
    print(f"ğŸ¤– AI Cleaning: ENABLED")
    print(f"ğŸ·ï¸ Unique Persian Title Generation: ENABLED")
    print(f"ğŸ”‘ Groq API: {'âœ… ENABLED' if groq_client else 'âŒ DISABLED (check GROQ_API_KEY)'}")
    print("=" * 60)

    download_instagram_profile(username, browser, max_posts)
    print("\nğŸ‰ AI-enhanced scraping with unique Persian title generation completed!")